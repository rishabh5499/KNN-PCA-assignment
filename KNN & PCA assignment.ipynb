{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a82859-1faf-45a7-a87e-9e255a217f0a",
   "metadata": {},
   "source": [
    "1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
    "   - K-Nearest Neighbors (KNN) is a supervised, instance-based (lazy learning) algorithm that makes predictions based on the K closest training samples to a new data point, using a distance metric like Euclidean distance.\n",
    "\n",
    "   - KNN for Classification\n",
    "   - Find the K nearest neighbors\n",
    "   - Take a majority vote\n",
    "   - The class with the highest count becomes the prediction\n",
    "   - Example: If K=5 and neighbors are [A, A, B, A, B] → output = A\n",
    "\n",
    "   - KNN for Regression\n",
    "   - Find the K nearest neighbors\n",
    "   - Predict the average (or weighted average) of their target values\n",
    "   - Example: targets [10, 12, 9, 11, 13] → output ≈ 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc153d9f-f0c6-4fad-82ff-ce83f79a6961",
   "metadata": {},
   "source": [
    "2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
    "   - The Curse of Dimensionality refers to problems that arise when the number of features (dimensions) becomes very large.\n",
    "\n",
    "   - Effect on KNN\n",
    "   - In high dimensions, points become far apart\n",
    "   - Distance measures become less meaningful (all distances look similar)\n",
    "   - KNN struggles to find “truly close” neighbors\n",
    "   - Model accuracy decreases and computation becomes expensive\n",
    "   - So, KNN performs best when features are few and meaningful, or after dimensionality reduction (like PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577d58a-b44e-4882-acb8-58ceb69cafd1",
   "metadata": {},
   "source": [
    "3. What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
    "   - PCA is an unsupervised dimensionality reduction technique that transforms the original features into new features called principal components.\n",
    "\n",
    "   - What PCA does\n",
    "   - Finds directions of maximum variance in data\n",
    "   - Projects data into fewer dimensions while keeping most information\n",
    "\n",
    "   - PCA vs Feature Selection\n",
    "   - PCA creates new features (components) and feature selection keeps original features.\n",
    "   - PCA uses linear comninations and Feature Selection chooses subset of existing columns.\n",
    "   - PCA is unsupervised and Feature Selection can be supervised.\n",
    "   - PCA improves noise handling and Feature Selection keeps interpretability of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03da251-676c-4b67-8862-2a7578670e1a",
   "metadata": {},
   "source": [
    "4. What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
    "   - In PCA, we compute the covariance matrix of the dataset.\n",
    "   - Eigenvectors represent the directions (axes) of maximum variance (principal components).\n",
    "   - Eigenvalues represent the amount of variance captured along each eigenvector.\n",
    "\n",
    "   - Why important?\n",
    "   - Eigenvectors decide the new coordinate system\n",
    "   - Eigenvalues decide which components are most useful\n",
    "   - Higher eigenvalue → more information retained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbdaee7-15af-4471-b443-8fa379452e36",
   "metadata": {},
   "source": [
    "5. How do KNN and PCA complement each other when applied in a single pipeline?\n",
    "   - KNN depends heavily on distance calculations. PCA helps by reducing dimensions and removing noise.\n",
    "\n",
    "   - Benefits of PCA + KNN\n",
    "   - Reduces Curse of Dimensionality\n",
    "   - Improves distance quality → better neighbors\n",
    "   - Faster training/prediction (less computation)\n",
    "   - Often improves accuracy on noisy/high-dimensional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997ff028-b69d-46da-a00d-50c97f19339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 0.8055555555555556\n",
      "Accuracy with scaling: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "# 6. Use the Wine Dataset from sklearn.datasets.load_wine().\n",
    "# Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc_no_scaling = accuracy_score(y_test, y_pred)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "acc_scaling = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(\"Accuracy without scaling:\", acc_no_scaling)\n",
    "print(\"Accuracy with scaling:\", acc_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8c1164-6c71-4aeb-9ecf-1fccbab6d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio:\n",
      "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
      " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
      " 0.00795215]\n"
     ]
    }
   ],
   "source": [
    "# 7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Explained variance ratio:\")\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11bd52d6-89f7-45e8-b00d-73d19b4464a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on original scaled data: 0.9722222222222222\n",
      "Accuracy on PCA (2 components): 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset. \n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "acc_original = accuracy_score(y_test, y_pred)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "print(\"Accuracy on original scaled data:\", acc_original)\n",
    "print(\"Accuracy on PCA (2 components):\", acc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc5df91-ca59-4971-bdc8-197d70e453b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean: 0.9722222222222222\n",
      "Accuracy with Manhattan: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results. \n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train_scaled, y_train)\n",
    "y_pred_e = knn_euclidean.predict(X_test_scaled)\n",
    "acc_e = accuracy_score(y_test, y_pred_e)\n",
    "\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train_scaled, y_train)\n",
    "y_pred_m = knn_manhattan.predict(X_test_scaled)\n",
    "acc_m = accuracy_score(y_test, y_pred_m)\n",
    "\n",
    "print(\"Accuracy with Euclidean:\", acc_e)\n",
    "print(\"Accuracy with Manhattan:\", acc_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d6020-4cf4-48af-b868-dae01aa60d4f",
   "metadata": {},
   "source": [
    "10. You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. \n",
    "Due to the large number of features and a small number of samples, traditional models overfit. \n",
    "Explain how you would: \n",
    "● Use PCA to reduce dimensionality \n",
    "● Decide how many components to keep \n",
    "● Use KNN for classification post-dimensionality reduction \n",
    "● Evaluate the model \n",
    "● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
    "\n",
    "    - Gene expression datasets usually have:\n",
    "    - Thousands of features (genes)\n",
    "    - Very few samples\n",
    "    - This causes overfitting and unstable models.\n",
    "\n",
    "    - Step 1: Use PCA to reduce dimensionality\n",
    "    - Standardize features first\n",
    "    - Apply PCA to compress thousands of genes into fewer components\n",
    "    - Removes noise and redundancy\n",
    "\n",
    "    - Step 2: Decide how many components to keep\n",
    "    - Use:\n",
    "    - Explained variance ratio\n",
    "    - Keep enough components to retain 95%–99% variance\n",
    "    - Example: choose n_components=0.95\n",
    "\n",
    "    - Step 3: Use KNN after PCA\n",
    "    - Train KNN on reduced feature space\n",
    "    - Use cross-validation to choose best K\n",
    "    - Distance becomes meaningful again due to lower dimensions\n",
    "\n",
    "    - Step 4: Evaluate the model\n",
    "    - Use:\n",
    "    - Train-test split + cross-validation\n",
    "    - Accuracy, Precision, Recall, F1-score\n",
    "    - Confusion matrix\n",
    "    - If dataset is imbalanced → use ROC-AUC\n",
    "\n",
    "    - Step 5: Justify pipeline to stakeholders\n",
    "    - Explain that:\n",
    "    - PCA reduces noise and prevents overfitting\n",
    "    - KNN becomes faster and more accurate in lower dimensions\n",
    "    - Pipeline is interpretable at a high level (variance retained)\n",
    "    - Cross-validation ensures reliability on real-world biomedical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a95c3-1dfa-438d-b0b9-82e16daddab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code for Q10\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# X_gene = gene expression matrix (samples x genes)\n",
    "# y = cancer type labels\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.95)),\n",
    "    (\"knn\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": [3, 5, 7, 9],\n",
    "    \"knn__metric\": [\"euclidean\", \"manhattan\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid.fit(X_gene, y)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best CV Score:\", grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gene, y, test_size=0.2, random_state=42)\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
